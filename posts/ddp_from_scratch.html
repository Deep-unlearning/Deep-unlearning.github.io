<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building PyTorch DDP From Scratch - Deep Unlearning</title>
    <link rel="stylesheet" href="../style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <nav>
            <a href="/">[home]</a>
            <a href="../blog.html">[blog]</a>
        </nav>

        <main>
            <article>
                <h1>Building PyTorch DDP From Scratch</h1>
                <span class="date">February 2, 2026</span>

                <p>This post is (hopefully) a first of a series of posts about distributed training. Today we will build a distributed data parallel (DDP) from scratch.</p>

                <p>The complete implementation is available on <a href="https://github.com/Deep-unlearning/ddp_from_scratch">GitHub</a>.</p>

                <h2>Motivation: Why Build DDP From Scratch?</h2>

                <p>DDP is the first step towards distributed training. Often neglicted by many people because of its simplicity. You just import <code>torch.distributed</code> modify few lines of code and run your training script with <code>torchrun</code>:</p>
               
                <p>Okay maybe it's a bit true, but we will see that there is more to it than that.
                In this post we will go from a naive implementation to an optimized one that can achieve up to <strong>95% of the performance of PyTorch DDP</strong>.</p>


                <h2>Sharded Data Loading</h2>

                <p>
                In Distributed Data Parallel (DDP) training, data is typically sharded across
                ranks so that each process works on a different subset of the dataset.
                This sharding is handled by the DataLoader and its sampler, not by DDP itself.
                </p>
                
                <p>
                In pytorch, this is commonly done using
                <code>torch.utils.data.distributed.DistributedSampler</code>, here we will implement a custom sampler that does essentially the same thing, you can find the implementation <a href="https://github.com/Deep-unlearning/ddp_from_scratch/blob/master/ddp/sampler.py">here</a>.
                </p>
                
                <p>The implementation is quite straightforward, the important things to remember are that:</p>
                
                <ul>
                  <li>
                    pytorch’s distributed sampler performs an interleaved (strided) slicing of
                    dataset indices. For example, with 4 ranks and 16 samples, rank 0 receives
                    <code>[0, 4, 8, 12]</code>, rank 1 receives <code>[1, 5, 9, 13]</code>,
                    rank 2 receives <code>[2, 6, 10, 14]</code>, and rank 3 receives
                    <code>[3, 7, 11, 15]</code>.
                  </li>
                
                  <li>
                    The <code>drop_last</code> parameter controls how the sampler handles cases
                    where the dataset size is not divisible by the number of ranks.
                    If <code>drop_last=True</code>, extra samples are dropped.
                    If <code>drop_last=False</code>, the sampler pads the list of dataset indices
                    by repeating from the beginning of the index list (after optional shuffling)
                    so that the total number of samples is divisible by the number of ranks.
                    For example, with 4 ranks and 15 samples (no shuffling), rank 0 receives
                    <code>[0, 4, 8, 12]</code>, rank 1 receives <code>[1, 5, 9, 13]</code>,
                    rank 2 receives <code>[2, 6, 10, 14]</code>, and rank 3 receives
                    <code>[3, 7, 11, 0]</code>.
                  </li>
                </ul>
                
                
                <h2>Naive Implementation</h2>

                <h2>Gradient Bucketing and Autograd Hooks</h2>

                <h2>Conclusion</h2>

                <a href="../blog.html" class="back-link">← Back to blog</a>
            </article>
        </main>

        <footer>
            <div class="social-links">
                <a href="https://github.com/Deep-unlearning" target="_blank">GitHub</a>
                <a href="https://linkedin.com" target="_blank">LinkedIn</a>
                <a href="https://twitter.com" target="_blank">Twitter</a>
            </div>
        </footer>
    </div>
</body>
</html>
